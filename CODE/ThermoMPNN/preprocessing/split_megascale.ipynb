{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662740ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from numpy import random\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0015faca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/srnwjfv172gg471lbzkr4j040000gp/T/ipykernel_9853/2575503434.py:1: DtypeWarning: Columns (30,31,36,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"../data/mega_scale/Processed_K50_dG_datasets/Tsuboyama2023_Dataset2_Dataset3_20230416.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(776298, 39)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/mega_scale/Processed_K50_dG_datasets/Tsuboyama2023_Dataset2_Dataset3_20230416.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6ab3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391090, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unreliable data points, insertions, deletions, and multiple mutations\n",
    "data = data.loc[data.ddG_ML != '-', :].reset_index(drop=True)\n",
    "data = data.loc[~data.mut_type.str.contains(\"ins\") & ~data.mut_type.str.contains(\"del\"), :].reset_index(drop=True)\n",
    "data = data.loc[~data.mut_type.str.contains(\"del\"), :].reset_index(drop=True)\n",
    "data = data.loc[~data.mut_type.str.contains(\":\"), :].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dabefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272712, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data.WT_name.unique()\n",
    "\n",
    "# check that each name has matching PDB somewhere\n",
    "pdb_dir = '../data/mega_scale/AlphaFold_model_PDBs'\n",
    "\n",
    "pdb_files = os.listdir(pdb_dir)\n",
    "names = [n for n in names if n in pdb_files]\n",
    "\n",
    "seqs = data.aa_seq.unique()\n",
    "df = data\n",
    "mut_rows, wt_seqs = {}, {}\n",
    "df['wt_seq'] = ''\n",
    "\n",
    "# make new wt_seq column to accompany each data point\n",
    "for wt_name in names:\n",
    "    wt_rows = df.query('WT_name == @wt_name and mut_type == \"wt\"').reset_index(drop=True)\n",
    "    mut_rows[wt_name] = df.query('WT_name == @wt_name and mut_type != \"wt\"').reset_index(drop=True)\n",
    "    wt_seqs[wt_name] = wt_rows.aa_seq[0]\n",
    "    df.loc[df['WT_name'] == wt_name, 'wt_seq'] = wt_rows.aa_seq[0]\n",
    "\n",
    "df = df.loc[df['WT_name'].isin(names), :]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7652a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put sequences into FASTA and cluster with mmseqs2\n",
    "\n",
    "# from Bio import SeqIO\n",
    "# from Bio.Seq import Seq\n",
    "\n",
    "# df = df.loc[df['WT_name'].isin(names), :]\n",
    "\n",
    "# records = {}\n",
    "# for r in df.to_records():\n",
    "#     name = r.WT_name\n",
    "#     if name not in records:\n",
    "#         records[name] = SeqIO.SeqRecord(Seq(r.wt_seq), id=name, name=name, description='')\n",
    "    \n",
    "# with open('../data/mega_scale/mega_proteins.fasta', 'w') as outFile:\n",
    "#     SeqIO.write(list(records.values()), outFile, 'fasta')\n",
    "\n",
    "# !mmseqs2 easy-cluster ../data/mega_scale/mega_proteins.fasta ../data/mega_scale/mega_proteins ../data/mega_scale/tmp --min-seq-id 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6327c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size:\t 272712\n",
      "Fold Size:\t (54962, 40)\n",
      "==================================================\n",
      "Fold Size:\t (54800, 40)\n",
      "==================================================\n",
      "Fold Size:\t (55266, 40)\n",
      "==================================================\n",
      "Fold Size:\t (55886, 40)\n",
      "==================================================\n",
      "Fold Size:\t (51798, 40)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "clust = pd.read_csv('../data/mega_scale/mega_proteins_cluster.tsv', sep='\\t', header=None, names=['cluster', 'member'])\n",
    "vcs = clust.cluster.value_counts()\n",
    "cs = clust.cluster.unique()\n",
    "\n",
    "# splitting into 5 cross-validation folds based on cluster IDs\n",
    "total_size = df.shape[0]\n",
    "random.seed(10)\n",
    "\n",
    "cv_df_list = []\n",
    "cv_folds = [0.2, 0.2, 0.2, 0.2, -1]\n",
    "used = []\n",
    "print('Total size:\\t', total_size)\n",
    "\n",
    "for fold in cv_folds:\n",
    "\n",
    "    fold_df = pd.DataFrame(columns=df.columns)\n",
    "    fold_size = fold_df.shape[0]\n",
    "    # loop until size is met:\n",
    "    if fold == -1:\n",
    "        # select all that remain\n",
    "        all_clusters = cs\n",
    "        rdf = df\n",
    "        for d in cv_df_list:\n",
    "            rdf = rdf[~rdf['WT_name'].isin(d['WT_name'])]  \n",
    "        print('Fold Size:\\t', rdf.shape)\n",
    "        print('=' * 50)\n",
    "        cv_df_list.append(rdf)\n",
    "        break\n",
    "    \n",
    "    while fold_size < total_size * fold:\n",
    "        pick = random.randint(0, len(cs))\n",
    "        if pick not in used:\n",
    "            used.append(pick)\n",
    "            cluster_picked = cs[pick]\n",
    "        else:\n",
    "            continue\n",
    "        # add all members of that cluster to fold_df\n",
    "        cluster_all = clust[clust['cluster'] == cluster_picked]\n",
    "        for c in cluster_all.member:\n",
    "            target_rows = df.loc[df['WT_name'] == c, :]\n",
    "            fold_df = pd.concat([fold_df, target_rows])\n",
    "        fold_size = fold_df.shape[0]\n",
    "    print('Fold Size:\\t', fold_df.shape)\n",
    "    print('=' * 50)\n",
    "    cv_df_list.append(fold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ead2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 59 51\n",
      "176 63 59\n",
      "172 63 63\n",
      "173 62 63\n",
      "185 51 62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'val', 'test', 'train_s669', 'cv_train_0', 'cv_val_0', 'cv_test_0', 'cv_train_1', 'cv_val_1', 'cv_test_1', 'cv_train_2', 'cv_val_2', 'cv_test_2', 'cv_train_3', 'cv_val_3', 'cv_test_3', 'cv_train_4', 'cv_val_4', 'cv_test_4'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assembling train/val/test splits based on random combinations of the 5 folds\n",
    "\n",
    "train_list = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 5], \n",
    "    [1, 4, 5], \n",
    "    [3, 4, 5],\n",
    "    [2, 3, 4]\n",
    "]\n",
    "\n",
    "test_list = [5, 4, 3, 2, 1]\n",
    "val_list = [4, 3, 2, 1, 5]\n",
    "\n",
    "\n",
    "with open('../data/mega_scale/mega_splits.pkl', 'rb') as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "n = 0\n",
    "\n",
    "for tr, te, val in zip(train_list, test_list, val_list):\n",
    "    test_val = cv_df_list[te - 1].WT_name.unique()\n",
    "    val_val = cv_df_list[val - 1].WT_name.unique()\n",
    "    train_val = [cv_df_list[t - 1].WT_name.unique() for t in tr]\n",
    "    train_val = list(itertools.chain.from_iterable(train_val))\n",
    "    print(len(train_val), len(val_val), len(test_val))\n",
    "    assert len(train_val) + len(val_val) + len(test_val) == 298\n",
    "    splits[f\"cv_train_{n}\"] = train_val\n",
    "    splits[f\"cv_val_{n}\"] = val_val\n",
    "    splits[f\"cv_test_{n}\"] = test_val\n",
    "    n += 1\n",
    "\n",
    "# with open('../data/mega_scale/mega_splits.pkl', 'wb') as f:\n",
    "#     pickle.dump(splits, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a50b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Size:\t 28312\n",
      "==================================================\n",
      "Validation Dataset Size:\t 27481\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# doing non-cross-validation train/val/test split for main ablation study runs\n",
    "\n",
    "# load tsv with homology overlap between MegaScale and FireProt\n",
    "overlap = pd.read_csv('../data/mmseqs_searches/mega_vs_fireprot.m8', sep='\\t', header=None)\n",
    "mega_overlap = overlap.iloc[:, 0].values\n",
    "fp_overlap = overlap.iloc[:, 1].values\n",
    "\n",
    "test_size, val_size, train_size = 0, 0, 0\n",
    "total_size = df.shape[0]\n",
    "\n",
    "test_df = pd.DataFrame(columns=df.columns)\n",
    "test_size = test_df.shape[0]\n",
    "random.seed(1)\n",
    "\n",
    "used = []\n",
    "# add test proteins until minimum size cutoff is met:\n",
    "while test_size < total_size * 0.10:\n",
    "    # pick random cluster\n",
    "    pick = random.randint(0, len(cs))\n",
    "    if pick not in used:\n",
    "        used.append(pick)\n",
    "        cluster_picked = cs[pick]\n",
    "    else: # avoid duplicate picks\n",
    "        continue\n",
    "    # pick a random cluster\n",
    "\n",
    "    # add all members of that cluster to train_df\n",
    "    cluster_all = clust[clust['cluster'] == cluster_picked]\n",
    "    \n",
    "    # check if any cluster members are in restricted list (overlap w/FireProt)\n",
    "    for c in cluster_all.member:\n",
    "        if c in mega_overlap or c in fp_overlap:\n",
    "            break\n",
    "    \n",
    "    if c in mega_overlap or c in fp_overlap:  # skip adding rows if held in overlap\n",
    "        continue\n",
    "    \n",
    "    for c in cluster_all.member:\n",
    "        target_rows = df.loc[df['WT_name'] == c, :]\n",
    "        test_df = pd.concat([test_df, target_rows])\n",
    "        test_size = test_df.shape[0]\n",
    "\n",
    "print('Test Dataset Size:\\t', test_size)\n",
    "print('=' * 50)\n",
    "    \n",
    "# repeat loop on validation set\n",
    "val_df = pd.DataFrame(columns=df.columns)\n",
    "val_size = val_df.shape[0]\n",
    "\n",
    "while val_size < total_size * 0.10:\n",
    "    # pick random cluster\n",
    "    pick = random.randint(0, len(cs))\n",
    "    if pick not in used:\n",
    "        used.append(pick)\n",
    "        cluster_picked = cs[pick]\n",
    "    else:  # avoid duplicate picks\n",
    "        continue\n",
    "    # add all members of that cluster to val_df\n",
    "    cluster_all = clust[clust['cluster'] == cluster_picked]\n",
    "    \n",
    "    # check if any cluster members are in restricted list (overlap w/FireProt)\n",
    "    for c in cluster_all.member:\n",
    "        if c in mega_overlap or c in fp_overlap:\n",
    "            break\n",
    "    \n",
    "    if c in mega_overlap or c in fp_overlap:  # skip adding rows if held in overlap\n",
    "        continue\n",
    "    \n",
    "    for c in cluster_all.member:\n",
    "        target_rows = df.loc[df['WT_name'] == c, :]\n",
    "        val_df = pd.concat([val_df, target_rows])\n",
    "        val_size = val_df.shape[0]\n",
    "\n",
    "print('Validation Dataset Size:\\t', val_size)\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "211a7551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 216919\n"
     ]
    }
   ],
   "source": [
    "val_names, test_names = val_df.WT_name.unique(), test_df.WT_name.unique()\n",
    "all_names = df.WT_name.unique()\n",
    "\n",
    "train_names = [a for a in all_names if a not in val_names and a not in test_names]\n",
    "train_df = df.loc[df['WT_name'].isin(train_names), :]\n",
    "print('Training Dataset Size:', train_df.shape[0])\n",
    "\n",
    "splits = {'train': train_names, 'val': val_names, 'test': test_names}\n",
    "\n",
    "# save splits to pickle file\n",
    "# with open('../data/mega_scale/mega_splits.pkl', 'wb') as f:\n",
    "#     pickle.dump(splits, f)\n",
    "\n",
    "# if desired, save split df to separate files (not needed for training)\n",
    "# train_df.to_csv('../data/mega_scale/Processed_K50_dG_datasets/mega_train.csv')\n",
    "# val_df.to_csv('../data/mega_scale/Processed_K50_dG_datasets/mega_val.csv')\n",
    "# test_df.to_csv('../data/mega_scale/Processed_K50_dG_datasets/mega_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
